If you had only the text description available, you could use an RNN or a 1D convnet.

If you had only picture, you could use a 2D convnet. 

Inception module
1. Input is processed by several parallel convolutional branches whose outputs are then merged back into a single tensor.
2.Recently, there is trend of adding residual connections to a model.
3. A residual connection consists of reinjecting previous representations into the downstream flow of data by adding a past output tensor to a later output tensor, which helps prevent information loss along the data-processing flow.

Functional API
1. Build input and output layers and then pass them to the keras_model function

Multi-input models
1. The functional API can be used to build models that have multiple inputs.
2. Such models at some point merge their different input branches using a layer that can combine several tensors: by adding them, concatenating them and so on. 

Very imbalanced loss contributions will cause the model representations to be optimized preferentially for the task with the largest individual loss, at the expense of the other tasks. 
To remedy this, you can assign different levels of importance to the loss values in their contribution to the final loss. 
This is useful in particular if the losses' values use different scales

Neural networks in Keras are allowed to be arbitrary directed acyclic graphs of layers.

Two notable neural-network components are implemented as graphs:
1.Inception modules
2.Residual connections


Inception
1. It is a popular type of network architecture for convolutional neural networks
2. It consists of a stack of modules that themselves look like small independent networks, split into several parallel braches
3. The most basic form of an Inception module has 3 to 4 braches starting with a 1x1 convolution, followed by a 3x3 convolution and ending with the concatenation of the resulting features.
4. This setup helps the network separately learn spatial features and channel-wise features, which is more efficient than learning them jointly

The purpose of 1x1 convolutions
1. Also called pointwise convolutions
2. Convolutions extract spatial patches around every tile in an input tensor and apply the same transformation to each patch.
3. An edge case is when the patches extracted consist of a single tile. 
4. The convolution operation then becomes equivalent to running each tile vector through a dense layer: it will compute feature that mix together information from the channels of the input tensor, but it won't mix information across space.
5. Pointwise convolutions are featured in Incpetion modules, where they contribute to factoring out channel-wise feature learning the spacewise feature learning-a reasonable thing to do if you assume that each channel is highly autocorrelated across space, but different channels may not be highly correlated with each other

Xception
1. It stands for extreme inception, is a convnet architecture loosely inspired by incpetion
2. It takes the idea of separating the learning of channel-wise and space-wise features to its logical extreme, and replaces Inception modules with depthwise separable convolutions consisting of a depthwise convolution followed by a pointwise convolution effectively, an extreme form of an Incpetion module, wher espatial features and channel-wise features are fully separated.
3. Xcpetion has rougly the same number of parameters as Inception V3, but it shows better runtime performance and higher accuracy on ImageNet as well as other largescale datasets, due to a more efficient use of model parameters.

Residual connections
1. Residual connections are a common graph-like network component
2. They tackle two common problems that plague any large-scale deep-learning model: vanishing gradients and representational bottlenecks.
3. In general, adding residual connections to any model that has more than 10 layers is likely to be beneficial. 

Layer weight sharing
1. Function API can reuse a layer instance several times
2. This allows you to build models that have shared branches-several branches that all share the same knowledge and perform the same operations. 
3. They share the same representations and learn these representations simultaneously for different sets of inputs
4. A residual connection consists of making the output of an earlier layer available as input to a later layer, effectively creating a shortcut in a sequential network. 
5. The earlier output is summed with the later activation, which assumes that both activations are the same size. 
6. If they're differnt sizes, you can use a linear transformation to reshape the earlier activation into the target shape. 

Vanishing gradients in deep learning
1. Backpropagation works by propagating a feedback signal from the output loss down to earlier layers
2. If this feedback signal has to be propagated through a deep stack of layers, the signal may become tenuous or even be lost entirely, rendering the network untrainable.

Carry track
1. It propagates information parallel to the main processing track. 
2. Residual connections work in a similar way in feedforward deep networks, but they're even simpler: they introduce a purely linear information carry track parallel to the main layer stack, thus helping to propagate gradients through arbitrarily deep stacks of layers

Siamese LSTM/Shared LSTM
1. The two input sentences are interchangeable, because semantic similarity is a symmetrical relationship: the similarity of A to B is identical to the similarity of B to A
2. Process both with a single LSTM layer. The representations of this lSTM layer are learned based on both inputs simultaneously. 


