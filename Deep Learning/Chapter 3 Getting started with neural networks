Common use of neural networks: binary classification, multiclass classification, and scalar regression

Layer: A layer is a data-processing module that takes as input one or more tensors and that outputs one or more tensors. Some layers are stateless, but more frequently layers have a state: the layer's weight, one or several tensors learned with stochastic gradient descent, which together contain the network knowledge

Vector data stored in 2D tensors of shape is often processed by densely connected layers, also called fully connected or dense layers

Sequence data, store in 3D tensors of shape, is typically processed by recurrent layers.

Image data, stored in 4D tensors, is usually processed by 2D convolution layers

A deep-learning model is a directed ,acyclic graph of layers. The most common instance is a linear stack of layers, mapping a single input to a single output. 

A neural network that has multiple outputs may have multiple loss functions. But the gradient-descent process must be based on a single scalar loss value; For multiloss networks, all losses are combined into a single scalar quantity

c(c(train_data, train_labels),c(test_data, test_labels))%<-% imdb
=
train_data<-imdb$train$x
train_labels<-imdb$train$y
test_data<-imdb$test$x
test_labels<-imdb$test$y

relu activation implments the chain of tensor operations: output=relu(dot(W, input)+b)

Final layer will use a sigmoid activation so as to output a probability (between 0 to 1).

Activation function: relu, prelu, elu

loss function: 
1.binary_crossentropy for output probabilities
2.Crossentropy is a quantity from the field of Informatino Theory that measures the distance between probability distributions. 

Optimizer:
1. rmsprop

The network ends with a single unit and no activation is a typical setup for scalar regression. 
