Supervised learning:
1.It consists of learning to map input data to known targets, given a set of examples. 
2.Almost all applications of deep learning that are in the spotlight these days belong in this category
3.Sequence genration: Given a picture, predict a caption describing it. Sequence generation can sometimes be reformulated as a series of classification problems
4. Syntax tree prediction: Given a sentence, predict its decomposition into a syntax tree
5.Object detection: Given a picture, draw a bounding box around certain objects inside the picture. This can be expressed as a classification problem or as a joint classification and regression problem, where the bounding-box coordinates are predicted via vector regression
6.Image segmentation: Given a picture, draw a pixel-level mask on a specific object

Unsuperived learning:
1. It consists of finding interesting transformations of the input data without the help of any targets, for the purposes of data visualization, data compression, or data denoising, or to better understand the correlations present in the data at hand.
2.It's often a necessary step in better understanding a dataset before attempting to solve a supervised-learning problem. 
3.Dimensionality reduction and clustering are well-known categories of unsupervised learning

Self-supervised learning
1. It is supervised learning without human-annotated labels.
2. A supervised learning without any humans in the loop.
3. There are still labels involved, but they are generated from the input data, typically using a heuristic algorithm
4. Autoencoders are a well-known example of self-supervised learning, where the generated targets are the input, unmodified. 

Reinforcement learning:
1. In reinforcement learning ,an agent receives information about its environment and learns to choose actions that will maximize some reward.
2. Currently, reinforcement learning is mostly a research area and hasn't yet had significant practical successes beyond games

Prediction error or loss value: a measure of the idstnace between your model's prediction and the target

Ground-truth or annotations: all targets for a dataset, typically collected by humans

Scalar regression: A task where the target is a continuous scalar value. 

Vector regression: A task where the target is a set of continuous values, a continuous vector. 

Mini-batch or batch: A small set of samples(8~128) that are processed sinultaneously by the model. 

Validation set:
1. Developing a model always involves tuning its configuration
2. You do this tuning by using as a feedback signal the performance of the model on the validation data. 
3. This tuning is a form of learning: a search for a good configuration in some parameter space. 
4. As a result, runing the configuration of the model based on its performance on the validation set can quickly result in overfitting to the validation set

Data preprocessing: Data proprocessing aims at making the raw data at hand more amenable to neura lnetworks
1. Vectorization
2. Normalization
3. Handling missing values
4. Feature extraction

Vectorization:
All inputs and targets in a neural network must be tensors of floating-point data. You must first turn into tensors, a step called data vectorization. 

Value normalization:
1. Take small values: typically most values should be in the 0-1 range
2. Be homogeneous: All features should take values in rougly the same range
3. Normalize each feature independently to have a mean of 0 (Stricter rule)
4. Normalize each feature independently to have a standard deviation of 1
5. scale(x)(Stricter rule)

Missing value
1. It's safe to input missing values as 0. The network will learn from exposure to the data that the value 0 means missing data and will start ignoring the value
2. If there are missing values in the test data, but the network was trained on data without any missing values, the network won't have learned to ignore missing values. 

Feature engineering
1. Feature engineering is the process of using your own knowledge about the data and about the machine-learning algorithm at thand to make the algorithm work better by applying hardcoded transformations to the data before it goes into the mdoel. 
2. Modern deep learning removes the need for most feature engineering, because neural networks are capable of automatically extracting useful features from raw data. 
3. Good features still allow you to solve problems more elegantly while using fewer resources. 
4. Good features let you solve a problem with far less data. The ability of deep-learning models to learn features on their own relies on having lots of training data available. 

Overfitting vs Underfitting
1. The fundamental issue in machine learning is the tension between optimization and generalization
2. Optiization refers to the process of adjusting a model to get the best performance possible on the training data, whereas generalization refers to how well the trained model performs on data it has never seen before. The goal of the game is to get good generalization but you don't control generalization. You can only adjust the model based on its training data.
3. Underfitting: The lower the loss on training data, the lower th eloss on test data
4. To prevent a model from learning misleading or irrelevant patterns found in the training data, the best solution is to get more training data. 
5. The process of fighting overfitting this way is called regularization. 

Reduceing the network's size
1. The simplest way to prevent overfitting is to reduce th esize of the model
2. In deep learning, the number of learnable parameters in a model is often referred to as the model's capacity. A model with more parameters has more memorization capacity and therefore can easily learn a perfect dictionary. 

Regularization
1.Weight regularization: A common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights to take only small values, which makes the distribution of weight values more regular.It's done by adding to the loss function of the network a cost associated with having large weights.
2. L1 regularization: The cost added is proportional to the absolute value of the weight coefficients
3. L2 regularization: The cost added is proportional to the square of the value of the weight coefficients. L2 regularization is also called weight decay in the context of neural networks.

Dropout
1. Dropout is one of the most effective and most commonly used regularization techniques for neural networks. 
2. Dropout, applied to a layer, consists of randomly dropping out(setting to zero) a number of output features of the layer during training. 
3. The dropout rate is the fraction of the features that are zeored out. It's usually set between 0.2 and 0.5. 


